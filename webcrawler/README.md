# Web Crawler API
Este proyecto proporciona un web crawler con una API REST para extraer y procesar contenido de pÃ¡ginas web, convirtiÃ©ndolo a formato Markdown.

## CaracterÃ­sticas

- ğŸŒ Crawling de pÃ¡ginas web con profundidad configurable.
- ğŸš€ API REST implementada con FastAPI.
- ğŸ“ ConversiÃ³n automÃ¡tica a formato Markdown.
- ğŸ“š DocumentaciÃ³n automÃ¡tica accesible a travÃ©s de Swagger UI.

## Estructura del Proyecto

```
webcrawler/
â”œâ”€â”€ webcrawler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ webcrawler.py
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Requisitos

- Python 3.8 o superior.
- pip (gestor de paquetes de Python).

## InstalaciÃ³n

1. Clona el repositorio:
   ```bash
   git clone https://github.com/usuario/webcrawler-api.git
   cd webcrawler-api
   ```

2. Crea un entorno virtual (opcional pero recomendado):
   ```bash
   python -m venv venv
   source venv/bin/activate  # En Windows: venv\Scripts\activate
   ```

3. Instala las dependencias:
   ```bash
   pip install -r requirements.txt
   ```

## Uso

### Ejecutar el API

Para iniciar el servidor, navega al directorio de la API y ejecuta:
```bash
cd api
uvicorn main:app --reload
```
El servidor se iniciarÃ¡ en `http://localhost:8000`.

### DocumentaciÃ³n del API

Accede a la documentaciÃ³n interactiva en:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### Ejemplo de Uso

Realiza una peticiÃ³n al API utilizando Python:

```python
import requests

url = "http://localhost:8000/crawl"
data = {
    "url": "https://ejemplo.com",
    "max_depth": 2
}

response = requests.post(url, json=data)
print(response.json())
```

TambiÃ©n puedes usar `curl`:
```bash
curl -X POST "http://localhost:8000/crawl" \
     -H "Content-Type: application/json" \
     -d '{"url":"https://ejemplo.com","max_depth":2}'
```

### Usar el Crawler Directamente

Puedes utilizar el crawler directamente en tu cÃ³digo:

```python
from webcrawler import crawl

content = crawl("https://ejemplo.com", max_depth=2)
print(content)
```

## ConfiguraciÃ³n

El API puede configurarse mediante variables de entorno:
- `PORT`: Puerto del servidor (por defecto: 8000).
- `HOST`: Host del servidor (por defecto: 0.0.0.0).
